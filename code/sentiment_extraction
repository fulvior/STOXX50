import os
import pymupdf
import re
import nltk
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
from nltk.tokenize import sent_tokenize
from collections import Counter
import pandas as pd

nltk.download("punkt")


def extract_text_from_pdf(pdf_path):
    doc = pymupdf.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text


def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def chunk_text(text, max_tokens=500):
    sentences = sent_tokenize(text)
    chunks, chunk, length = [], [], 0
    for sentence in sentences:
        tokens = sentence.split()
        if length + len(tokens) > max_tokens:
            chunks.append(" ".join(chunk))
            chunk, length = [], 0
        chunk.append(sentence)
        length += len(tokens)
    if chunk:
        chunks.append(" ".join(chunk))
    return chunks


def load_finbert():
    model = BertForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")
    tokenizer = BertTokenizer.from_pretrained("yiyanghkust/finbert-tone")
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)


def analyze_sentiment(chunks, sentiment_pipeline):
    results = []
    for chunk in chunks:
        sentiment = sentiment_pipeline(chunk[:512])[0]
        results.append(sentiment['label'].lower())
    return results


def aggregate_sentiment(sentiments):
    counts = Counter(sentiments)
    total = sum(counts.values())
    for label in counts:
        counts[label] = round((counts[label] / total) * 100, 2)
    return counts


def process_folder(folder_path):
    sentiment_pipeline = load_finbert()
    summary_data = []

    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            file_path = os.path.join(folder_path, filename)
            print(f"Processing: {filename}")
            try:
                raw_text = extract_text_from_pdf(file_path)
                cleaned_text = clean_text(raw_text)
                chunks = chunk_text(cleaned_text)
                sentiments = analyze_sentiment(chunks, sentiment_pipeline)
                aggregated = aggregate_sentiment(sentiments)
                aggregated['filename'] = filename
                summary_data.append(aggregated)
            except Exception as e:
                print(f"Error processing {filename}: {e}")

    return summary_data


def save_to_csv(data, output_file):
    df = pd.DataFrame(data).fillna(0)  # Fill missing sentiment classes with 0
    df = df[["filename", "positive", "neutral", "negative"]]
    df.to_csv(output_file, index=False)
    print(f"\nResults saved to {output_file}")


if __name__ == "__main__":
    folder_path = "/kaggle/input/reports"  # ‚Üê Replace with your folder
    results = process_folder(folder_path)
    save_to_csv(results, "sentiment_summary.csv")
